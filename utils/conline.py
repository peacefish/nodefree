import requests
from bs4 import BeautifulSoup
import urllib3
import re
from datetime import datetime
from requests.packages.urllib3.exceptions import InsecureRequestWarning
import logging
import cloudscraper
import yaml
scraper = cloudscraper.create_scraper(delay=5, browser={
    'browser': 'chrome',
    'platform': 'linux',
    'mobile': False,
})

# Enable debug-level logging
logging.basicConfig(level=logging.DEBUG)
# ç¦ç”¨ä¸å®‰å…¨è¯·æ±‚è­¦å‘Š
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)
headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
      'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
      # å…¶ä»–è¯·æ±‚å¤´å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ 
  }                                           
def extract_links(url):
    # å‘é€HTTPè¯·æ±‚
    response = scraper.get(url,proxies={'http': 'http://39.102.209.163:2128'})
    #print(response.text)
    # ç¡®ä¿è¯·æ±‚æˆåŠŸ
    if response.status_code == 200:
        # ä½¿ç”¨BeautifulSoupè§£æHTMLå†…å®¹
        soup = BeautifulSoup(response.text, 'html.parser')
        # æ‰¾åˆ°æ‰€æœ‰çš„<a>æ ‡ç­¾
        links = soup.find_all(class_="ceo-cover-container")
        # æå–hrefå±æ€§
        extracted_links = [link.get('href') for link in links if link.get('href') is not None]
        #print(extracted_links)
        return extracted_links
    else:
        return "Failed to retrieve the webpage"
def extract_links_url(url):
    # å‘é€HTTPè¯·æ±‚
    response = scraper.get(url,proxies={'http': 'http://39.102.209.163:2128'})
    # ç¡®ä¿è¯·æ±‚æˆåŠŸ
    if response.status_code == 200:
        # ä½¿ç”¨BeautifulSoupè§£æHTMLå†…å®¹
        soup = BeautifulSoup(response.text, 'html.parser')
        # æ‰¾åˆ°æ‰€æœ‰çš„<a>æ ‡ç­¾
        links = soup.find_all('a', href=lambda href: href and href.endswith('.yaml'))
        # æå–hrefå±æ€§
        extracted_links = [link.get('href') for link in links if link.get('href') is not None]
        return extracted_links
    else:
        return "Failed to retrieve the webpage"


def main():
    file1 = open("./sub/clash_online.yaml", "w+",encoding='utf-8')
    # è·å–å½“å‰æ—¥æœŸ
    current_date = datetime.now()

    url = "https://www.85la.com/internet-access/free-network-nodes"
    links = extract_links(url)
    a=extract_links_url(links[0])
    response = scraper.get(a[0],proxies={'http': 'http://39.102.209.163:2128'})
    #print(response.text)
    response.encoding = 'utf-8'
    dct = yaml.safe_load(response.text.replace('ğŸ‘‰','').replace('www.85la.com','').replace('()','').replace('( https://www.fuye.fun/å…è´¹èŠ‚ç‚¹åˆ†äº«)',''))
    a=dct['proxies']
    yaml_content = yaml.dump({"proxies":a}, default_flow_style=False)
    file1.write(yaml_content)
    file1.close()
    print("ok url")

def download_content(url):
    """
    ç¬¬ä¸€ä¸ªå‡½æ•°ï¼Œç”¨æ¥ä¸‹è½½ç½‘é¡µï¼Œè¿”å›ç½‘é¡µå†…å®¹
    å‚æ•° url ä»£è¡¨æ‰€è¦ä¸‹è½½çš„ç½‘é¡µç½‘å€ã€‚
    æ•´ä½“ä»£ç å’Œä¹‹å‰ç±»ä¼¼
    """
    response = requests.get(url)
    response.encoding = 'utf-8' 
    #print(response)
    text=response.text.replace('ğŸ‘‰','').replace('www.85la.com','').replace('()','').replace('( https://www.fuye.fun/å…è´¹èŠ‚ç‚¹åˆ†äº«)','')
    return text

if __name__ == '__main__':
    main()
